{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infinite pool QBC\n",
    "\n",
    "### Import packages\n",
    "**Keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Lambda\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import History, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import initializers\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Science**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import scipy.stats as stats\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn\n",
    "print(sklearn.__version__)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from math import floor\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Configuration regarding dimensionality\n",
    "Create a dataframe containing the available parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = pd.DataFrame(columns=[\n",
    "    'parameter',     # Parameter name\n",
    "    'minimum',       # Minimum value for sampling\n",
    "    'maximum',       # Maximum value for sampling\n",
    "    'default',       # Parameter value if this parameter is fixed\n",
    "    'is_slepton',    # Boolean: indicates if it is part of the slepton sector (at leading order)\n",
    "    'is_ew',         # Boolean: indicates if it is part of the electroweak sector (at leading order)\n",
    "    'is_higgs',      # Boolean: indicates if it is part of the higgs sector (at leading order)\n",
    "    'is_thirdgen',   # Boolean: indicates if it is part of the third generation sector (at leading order)\n",
    "    'is_squark',     # Boolean: indicates if it is part of the squark sector (at leading order)\n",
    "    'is_gluino'      # Boolean: indicates if it is part of the gluino sector (at leading order)\n",
    "])\n",
    "\n",
    "def add_row(df, name, minimum, maximum, default, types):\n",
    "    df = df.append({\n",
    "        'parameter': name,\n",
    "        'minimum': minimum, \n",
    "        'maximum': maximum,\n",
    "        'default': default,\n",
    "        'is_slepton': 'slepton' in types,\n",
    "        'is_ew': 'ew' in types,\n",
    "        'is_higgs': 'higgs' in types,\n",
    "        'is_thirdgen': 'thirdgen' in types,\n",
    "        'is_squark': 'squark' in types,\n",
    "        'is_gluino': 'gluino' in types}, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "parameters = add_row(parameters, 'M1',      -4000,  4000,     1750,     ['ew'])\n",
    "parameters = add_row(parameters, 'M2',      -4000,  4000,     1750,     ['ew'])\n",
    "parameters = add_row(parameters, 'M3',       200,   4000,     1750,     ['gluino'])\n",
    "parameters = add_row(parameters, 'mL1',      90,    4000,     1750,     ['slepton'])\n",
    "parameters = add_row(parameters, 'mL3',      90,    4000,     1750,     ['slepton'])\n",
    "parameters = add_row(parameters, 'mE1',      90,    4000,     1750,     ['slepton'])\n",
    "parameters = add_row(parameters, 'mE3',      90,    4000,     1750,     ['slepton'])\n",
    "parameters = add_row(parameters, 'mQ1',      200,   4000,     1750,     ['squark'])\n",
    "parameters = add_row(parameters, 'mQ3',      100,   4000,     1750,     ['squark', 'thirdgen'])\n",
    "parameters = add_row(parameters, 'mU1',      200,   4000,     1500,     ['squark'])\n",
    "parameters = add_row(parameters, 'mU3',      100,   4000,     3000,     ['squark', 'thirdgen'])\n",
    "parameters = add_row(parameters, 'mD1',      200,   4000,     2000,     ['squark'])\n",
    "parameters = add_row(parameters, 'mD3',      100,   4000,     2000,     ['squark', 'thirdgen'])\n",
    "parameters = add_row(parameters, 'At',      -8000,  8000,     3200,     ['thirdgen', 'higgs'])\n",
    "parameters = add_row(parameters, 'Ab',      -4000,  4000,     2000,     ['thirdgen'])\n",
    "parameters = add_row(parameters, 'Atau',    -4000,  4000,     2000,     ['slepton'])\n",
    "parameters = add_row(parameters, 'mu',      -4000,  4000,     200,      ['ew', 'higgs'])\n",
    "parameters = add_row(parameters, 'MA2',      10000, 16000000, 10000000, ['higgs'])\n",
    "parameters = add_row(parameters, 'tanbeta',  1,     60,       10,       ['ew', 'higgs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dataframe, we can select which sampling parameters to use and which parameters to fix. We select these based on the sectors: by excluding sectors from our sampling we can decrease the dimensionality of the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_free_parameters(df, fixed_sectors):\n",
    "    # Get rows to fix\n",
    "    fixed_rows = pd.Series([False]*19)\n",
    "    for f in fixed_sectors:\n",
    "        fixed_rows = fixed_rows | df['is_'+f]\n",
    "    # Return Series indicating which variables to sample\n",
    "    return ~fixed_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the dimensionality of the sampling space if we increase the number of sectors to remove from the parameter space. The ordering of the sectors is chosen such that the sectors with the weakest ALTAS limits are removed first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sectors = ['slepton', 'ew', 'higgs', 'thirdgen', 'squark', 'gluino']\n",
    "x = np.arange(len(sectors)+1)\n",
    "dimensionality = []\n",
    "\n",
    "for i in range(len(sectors)+1):\n",
    "    fixed_sectors = sectors[:i]\n",
    "    sample = get_free_parameters(parameters, fixed_sectors)\n",
    "    dimensionality.append( np.sum(1*sample) )\n",
    "\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.scatter(x, dimensionality)\n",
    "plt.xticks(x, (['-']+sectors))\n",
    "plt.yticks(np.arange(0, 24, step=4))\n",
    "for i, dim in enumerate(dimensionality):\n",
    "    plt.text(x[i]+0.05, dimensionality[i]-0.2, dim)\n",
    "plt.xlabel('Removed sectors (f.l.t.r. incremental)')\n",
    "plt.ylabel('Number of dimensions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sectors = ['thirdgen']\n",
    "sample = get_free_parameters(parameters, sectors)\n",
    "print(parameters[~sample][['parameter', 'default']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Functions\n",
    "**Plot Projection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_projection(data, classification, x=0, y=1, bins=100):\n",
    "    # Calculate ranges and bins\n",
    "    xmin, xmax = np.amin(data[:,x]), np.amax(data[:,x])\n",
    "    ymin, ymax = np.amin(data[:,y]), np.amax(data[:,y])\n",
    "    xbins = np.linspace(xmin, xmax, bins)\n",
    "    ybins = np.linspace(ymin, ymax, bins)\n",
    "    # Calculate two histograms\n",
    "    allowed, _, _ = np.histogram2d(data[classification==1.0, x], data[classification==1.0, y], [xbins, ybins])\n",
    "    excluded, _, _ = np.histogram2d(data[classification==0.0, x], data[classification==0.0, y], [xbins, ybins])\n",
    "    # Calculate map\n",
    "    mapping = allowed / (allowed + excluded)\n",
    "    mapping = mapping.T\n",
    "    mapping = np.flipud(mapping)\n",
    "    # Plot\n",
    "    f, a = plt.subplots(1,1,figsize=(8,8))\n",
    "    a.matshow(mapping, extent=(xmin, xmax, ymin, ymax), cmap=\"seismic_r\")\n",
    "    print(\"({}, {})\".format(np.amin(mapping),np.amax(mapping)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    # Normalize data\n",
    "    mu = (parameters.maximum.values + parameters.minimum.values)/2\n",
    "    sigma = (parameters.maximum.values - parameters.minimum.values)/np.sqrt(12)\n",
    "    data = (data - mu)/sigma\n",
    "    # Return\n",
    "    return data\n",
    "\n",
    "def undo_normalize(data):\n",
    "    # Normalize data\n",
    "    mu = (parameters.maximum.values + parameters.minimum.values)/2.0\n",
    "    sigma = (parameters.maximum.values - parameters.minimum.values)/np.sqrt(12)\n",
    "    data = data*sigma + mu\n",
    "    # Return\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oracle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oracle(data):\n",
    "    oracle_model = load_model(\"susyai.hdf5\")\n",
    "    y = 1.0*(oracle_model.predict(data)[:,1] > 0.5)\n",
    "    del(oracle_model)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N, parameters, sample_selection, normalized=True):\n",
    "    # Set fixed parameters\n",
    "    X = np.ones((N, len(parameters))) * parameters.default.values\n",
    "    # Set sampling parameters\n",
    "    X[:,sample_selection] = np.random.rand(N, np.sum(1*sample_selection))\n",
    "    X[:,sample_selection] *= (parameters[sample_selection].maximum - parameters[sample_selection].minimum).values\n",
    "    X[:,sample_selection] += parameters[sample_selection].minimum.values\n",
    "    X_normed = normalize(X)\n",
    "    # Get labels for points\n",
    "    y = oracle(X_normed).astype(np.float)\n",
    "    if normalized:\n",
    "        return (X_normed,y)\n",
    "    return (X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(Xtrain, ytrain):\n",
    "    est = RandomForestClassifier(n_estimators=200, n_jobs=-1)\n",
    "    est.fit(Xtrain, ytrain)\n",
    "    return est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, Xtest):\n",
    "    prediction = model.predict_proba(Xtest)[:,1]\n",
    "    #print(prediction)\n",
    "    info = 1 - 2*np.abs(prediction-0.5)\n",
    "    return (prediction, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Show model uncertainty results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uncertainty_results(X, truth, prediction, info_measure, x=0, y=2):\n",
    "    print(\"{:<20}{}\".format(\"truth.shape\", ytest.shape))\n",
    "    print(\"{:<20}{}\".format(\"pred.shape\", ypred.shape))\n",
    "    print(\"{:<20}{}\".format(\"info.shape\", ysigma.shape))\n",
    "\n",
    "    f, a = plt.subplots(2,2, figsize=(16,16))\n",
    "    a[0,0].scatter(X[:,x], X[:,y], c=truth.ravel(), cmap=\"seismic_r\")\n",
    "    a[0,0].set_title(\"Truth\")\n",
    "    a[0,1].scatter(X[:,x], X[:,y], c=prediction.ravel(), cmap=\"seismic_r\")\n",
    "    a[0,1].set_title(\"Prediction\")\n",
    "    a[1,0].scatter(X[:,x], X[:,y], c=np.abs(truth-prediction), cmap=\"Reds\")\n",
    "    a[1,0].set_title(\"Difference\")\n",
    "    a[1,1].scatter(X[:,x], X[:,y], c=info_measure, cmap=\"Purples\")\n",
    "    a[1,1].set_title(\"Uncertainty\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Setup experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#free = get_free_parameters(parameters, ['slepton', 'ew', 'higgs', 'thirdgen', 'squark'])\n",
    "#free = get_free_parameters(parameters, ['slepton', 'ew', 'higgs', 'thirdgen'])\n",
    "#free = get_free_parameters(parameters, ['slepton', 'ew', 'higgs'])\n",
    "#free = get_free_parameters(parameters, ['slepton', 'ew'])\n",
    "#free = get_free_parameters(parameters, ['slepton'])\n",
    "free = get_free_parameters(parameters, [])\n",
    "n_free = np.sum(1*free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 25\n",
    "x, y = generate_data(N, parameters, free, True)\n",
    "\n",
    "x = (x*256).astype(np.int)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "ax.matshow(x)\n",
    "\n",
    "for i,f in enumerate(free):\n",
    "    if not f:\n",
    "        p = patches.Rectangle((i-0.5, -.5), 1, N, fill=False, hatch='/////')\n",
    "        ax.add_patch(p)\n",
    "\n",
    "plt.xticks(np.arange(19), parameters.parameter.values.tolist(), rotation=90)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Active learning\n",
    "**Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_start = int(10000*((n_free/19)**2))       # Start size\n",
    "size_iter = int(2500*((n_free/19)**2))         # Number of data points added in each step\n",
    "size_sample = int(100000*((n_free/19)**2))     # Size of set to be checked for uncertainty\n",
    "size_max = int(100000*((n_free/19)**2))        # Maximum size of data set\n",
    "size_test = 1000000                            # Size of test set\n",
    "niterations = 7                                # Number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Active Sampling function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_sampling(model, Nquery, Nselect, random_fraction=0.5):\n",
    "    # Select actively\n",
    "    select_active = round((1-random_fraction)*Nselect)\n",
    "    if select_active > 0:\n",
    "        # Get uncertainty measure\n",
    "        X, _ = generate_data(Nquery, parameters, free)\n",
    "        predictions, info = test_model(model, X[:,free])   \n",
    "        keysort = np.argsort(info)[::-1]\n",
    "    \n",
    "        selected = X[keysort[:select_active]]\n",
    "        method = np.zeros(Nselect)\n",
    "        method[:select_active] = 1.0\n",
    "    else:\n",
    "        selected = None\n",
    "\n",
    "    # Add random\n",
    "    select_random = Nselect - select_active\n",
    "    if select_random > 0:\n",
    "        X, _ = generate_data(select_random, parameters, free)\n",
    "        if selected is None:\n",
    "            selected = X\n",
    "        else:\n",
    "            selected = np.vstack((selected, X))\n",
    "\n",
    "    # Label and return\n",
    "    prediction = oracle(selected)\n",
    "    return (selected, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get model performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(model, Xtest, ytest):\n",
    "    ypred = model.predict(Xtest)\n",
    "    bce = log_loss(ytest, ypred)\n",
    "    acc = accuracy_score(ytest, 1.0*(ypred>0.5))\n",
    "    return {\"bce\":bce, \"acc\":acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create logbooks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_AL = open(\"log_active_learning.csv\", \"w\")\n",
    "log_AL.write(\"iteration,size,bce,acc,dt\\n\")\n",
    "log_AL.flush()\n",
    "\n",
    "log_RS = open(\"log_random_sampling.csv\", \"w\")\n",
    "log_RS.write(\"iteration,size,bce,acc,dt\\n\")\n",
    "log_RS.flush()\n",
    "\n",
    "def log_result(log, iteration, size, bce, acc, dt):\n",
    "    log.write(\"{},{},{},{},{}\\n\".format(iteration, size, bce, acc, dt))\n",
    "    log.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run Active Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('arrays'):\n",
    "    os.mkdir('arrays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for iteration in range(niterations):\n",
    "    print(\"ITERATION {}\".format(iteration))\n",
    "\n",
    "    # Calculate time to sample all data randomly\n",
    "    Xtrain_RS, ytrain_RS = generate_data(size_max, parameters, free)\n",
    "    t_start_rs = time.time()\n",
    "    Xnew, ynew = generate_data(size_max, parameters, free)\n",
    "    Xtrain_RS = np.vstack((Xtrain_RS, Xnew))\n",
    "    ytrain_RS = np.hstack((ytrain_RS, ynew))\n",
    "    t_end_rs = time.time()\n",
    "    dt_random = t_end_rs - t_start_rs\n",
    "    \n",
    "    # Data set creation\n",
    "    Xtrain_AL, ytrain_AL = generate_data(size_start, parameters, free)\n",
    "    Xtrain_RS, ytrain_RS = deepcopy(Xtrain_AL), deepcopy(ytrain_AL)\n",
    "    Xtest, ytest = generate_data(size_test, parameters, free)\n",
    "    print(\"{} of {} points is allowed\".format(np.sum(ytest), len(ytest)))\n",
    "\n",
    "    while len(Xtrain_AL) < size_max:\n",
    "        K.clear_session()\n",
    "        \n",
    "        \"\"\" ACTIVE LEARNING \"\"\"\n",
    "        t_start_al = time.time()\n",
    "        # Train model for active learning\n",
    "        model = train_model(Xtrain_AL[:,free], ytrain_AL)\n",
    "        # Active sampling of new points\n",
    "        Xnew, ynew = active_sampling(model, size_sample, size_iter, random_fraction=0.0)\n",
    "        # Append new points to active learning\n",
    "        Xtrain_AL = np.vstack((Xtrain_AL, Xnew))\n",
    "        ytrain_AL = np.hstack((ytrain_AL, ynew))\n",
    "        t_end_al = time.time()\n",
    "        # Test model performance\n",
    "        performance_AL = model_performance(model, Xtest[:,free], ytest)\n",
    "        # Store performance\n",
    "        log_result(log_AL, iteration, len(Xtrain_AL)-size_iter, performance_AL[\"bce\"], performance_AL[\"acc\"], t_end_al-t_start_al)\n",
    "        \n",
    "        \"\"\" RANDOM SAMPLING \"\"\"\n",
    "        # Train model for random sampling\n",
    "        model = train_model(Xtrain_RS[:,free], ytrain_RS)\n",
    "        # Sample new points\n",
    "        Xnew, ynew = generate_data(size_iter, parameters, free)\n",
    "        # Append new points to random sampling\n",
    "        Xtrain_RS = np.vstack((Xtrain_RS, Xnew))\n",
    "        ytrain_RS = np.hstack((ytrain_RS, ynew))\n",
    "        # Test model performance\n",
    "        performance_RS = model_performance(model, Xtest[:,free], ytest)\n",
    "        # Store performance\n",
    "        log_result(log_RS, iteration, len(Xtrain_RS)-size_iter, performance_RS[\"bce\"], performance_RS[\"acc\"], dt_random)\n",
    "\n",
    "        \n",
    "        \"\"\" LOG AND OUTPUT RESULTS \"\"\"\n",
    "        # Screen\n",
    "        #print(Xtrain_AL.shape, Xtrain_RS.shape)\n",
    "        print(\"iteration: {:<5}     size: {:<5}    al-bce: {:<10}    al-acc: {:<10}    rs-bce: {:<10}    rs-acc: {:<10}\".format(\n",
    "            iteration,\n",
    "            len(Xtrain_AL)-size_iter,\n",
    "            round(performance_AL[\"bce\"],6),\n",
    "            round(performance_AL[\"acc\"],6),\n",
    "            round(performance_RS[\"bce\"],6),\n",
    "            round(performance_RS[\"acc\"],6)\n",
    "        ))\n",
    "    # Output data sets to file\n",
    "    al = np.hstack((Xtrain_AL, ytrain_AL.reshape(-1,1)))\n",
    "    rs = np.hstack((Xtrain_RS, ytrain_RS.reshape(-1,1)))\n",
    "    np.savetxt(\"arrays/active_{}.csv\".format(iteration), al, delimiter=\",\")\n",
    "    np.savetxt(\"arrays/random_{}.csv\".format(iteration), rs, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
